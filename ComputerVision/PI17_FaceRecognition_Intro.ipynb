{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3710jvsc74a57bd0491bb21bcabfba6e0f2b8a1cae5c9b38163f54254a79db2edc429af5b5926646",
   "display_name": "Python 3.7.10 64-bit ('tensor2': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# What is face recognition?\n",
    "In this tutorial, you will learn about face recognition, including:\n",
    "\n",
    "* How face recognition works\n",
    "* How face recognition is different from face detection\n",
    "* A history of face recognition algorithms\n",
    "* State-of-the-art algorithms used for face recognition today\n",
    "\n",
    "Face recognition is the process of taking a face in an image and actually identifying who the face belongs to. Face recognition is thus a form of person identification.\n",
    "\n",
    "Early face recognition systems relied on an early version of facial landmarks extracted from images, such as the relative position and size of the eyes, nose, cheekbone, and jaw. However, these systems were often highly subjective and prone to error since these quantifications of the face were manually extracted by the computer scientists and administrators running the face recognition software.\n",
    "\n",
    "As machine learning algorithms became more powerful and the computer vision field matured, face recognition systems started to utilize feature extraction and classification models to identify faces in images.\n",
    "\n",
    "Not only are these systems non-subjective, but they are also automatic — no hand labeling of the face is required. We simply extract features from the faces, train our classifier, and then use it to identify subsequent faces.\n",
    "\n",
    "Most recently, we’ve started to utilize deep learning algorithms for face recognition. State-of-the-art face recognition models such as FaceNet and OpenFace rely on a specialized deep neural network architecture called siamese networks.\n",
    "\n",
    "These neural networks are capable of obtaining face recognition accuracy that was once thought impossible (and they can achieve this accuracy with surprisingly little data).\n",
    "\n",
    "## How is face recognition different from face detection?\n",
    "Face detection and face recognition are distinctly different algorithms — face detection will tell you where in a given image/frame a face is (but not who the face belongs to) while face recognition actually identifies the detected face.\n",
    "\n",
    "\n",
    "In this tutorial, you will learn about face recognition, including:\n",
    "\n",
    "How face recognition works\n",
    "How face recognition is different from face detection\n",
    "A history of face recognition algorithms\n",
    "State-of-the-art algorithms used for face recognition today\n",
    "\n",
    "Next week we will start implementing these face recognition algorithms.\n",
    "\n",
    "To learn about face recognition, just keep reading.\n",
    "\n",
    "\n",
    "Looking for the source code to this post?\n",
    "JUMP RIGHT TO THE DOWNLOADS SECTION \n",
    "What is face recognition?\n",
    "Face recognition is the process of taking a face in an image and actually identifying who the face belongs to. Face recognition is thus a form of person identification.\n",
    "\n",
    "Early face recognition systems relied on an early version of facial landmarks extracted from images, such as the relative position and size of the eyes, nose, cheekbone, and jaw. However, these systems were often highly subjective and prone to error since these quantifications of the face were manually extracted by the computer scientists and administrators running the face recognition software.\n",
    "\n",
    "As machine learning algorithms became more powerful and the computer vision field matured, face recognition systems started to utilize feature extraction and classification models to identify faces in images.\n",
    "\n",
    "Not only are these systems non-subjective, but they are also automatic — no hand labeling of the face is required. We simply extract features from the faces, train our classifier, and then use it to identify subsequent faces.\n",
    "\n",
    "Most recently, we’ve started to utilize deep learning algorithms for face recognition. State-of-the-art face recognition models such as FaceNet and OpenFace rely on a specialized deep neural network architecture called siamese networks.\n",
    "\n",
    "These neural networks are capable of obtaining face recognition accuracy that was once thought impossible (and they can achieve this accuracy with surprisingly little data).\n",
    "\n",
    "How is face recognition different from face detection?\n",
    "\n",
    "Figure 1: Face recognition can be thought of as a two-step process. First, we must detect the presence of the face using a face detector and extract the face region of interest (ROI). Once we have the face ROI we can perform face recognition, the process of actually identifying who is in the image.\n",
    "I’ve often seen new computer vision and deep learning practitioners confuse the difference between face detection and face recognition, sometimes (and incorrectly) using the terms interchangeably.\n",
    "\n",
    "Face detection and face recognition are distinctly different algorithms — face detection will tell you where in a given image/frame a face is (but not who the face belongs to) while face recognition actually identifies the detected face.\n",
    "\n",
    "Let’s break this down a bit farther:\n",
    "\n",
    "Unlike face detection, which is the process of simply detecting the presence of a face in an image or video stream, face recognition takes the faces detected from the localization phase and attempts to identify whom the face belongs to. Face recognition can thus be thought of as a method of person identification, which we use heavily in security and surveillance systems.\n",
    "\n",
    "Since face recognition, by definition, requires face detection, we can think of face recognition as a two-phase process.\n",
    "\n",
    "* Phase #1: Detect the presence of faces in an image or video stream using methods such as Haar cascades, HOG + Linear SVM, deep learning, or any other algorithm that can localize faces.\n",
    "* Phase #2: Take each of the faces detected during the localization phase and identify each of them — this is where we actually assign a name to a face.\n",
    "\n",
    "## A brief history of face recognition\n",
    "\n",
    "This all changed in 1971 when Goldstein et al. published Identification of human faces. A crude first attempt at face identification, this method proposed 21 subjective facial features, such as hair color and lip thickness, to identify a face in a photograph.\n",
    "\n",
    "The largest drawback of this approach was that the 21 measurements (besides being highly subjective) were manually computed — an obvious flaw in a computer science community that was rapidly approaching unsupervised computation and classification (at least in terms of human oversight).\n",
    "\n",
    "Then, over a decade later, in 1987, Sirovich and Kirby published their seminal work, A Low-Dimensional Procedure for the Characterization of Human Faces which was later followed by Turk and Pentland in 1991 with Face Recognition Using Eigenfaces.\n",
    "\n",
    "<image src=\"images/what_is_face_reco_eigenfaces_samples.png\">\n",
    "\n",
    "Figure 3: The Eigenfaces algorithm is rooted in linear algebra and Principal Component Analysis (PCA) (image source).\n",
    "\n",
    "Both Sirovich and Kirby, along with Turk and Pentland, demonstrated that a standard linear algebra technique for dimensionality reduction called Principal Component Analysis (PCA) could be used to identify a face using a feature vector smaller than 100-dim.\n",
    "\n",
    "Furthermore, the “principal components” (i.e., the eigenvectors, or the “eigenfaces”) could be used to reconstruct faces from the original dataset. This implies that a face could be represented (and eventually identified) as a linear combination of the eigenfaces:\n",
    "\n",
    "Query Face = 36% of Eigenface #1 + -8% of Eigenface #2 … + 21% of Eigenface N"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Following the work of Sirovich and Kirby in the late 1980s, further research in face recognition exploded — another popular linear algebra-based face recognition technique utilized Linear Discriminant Analysis. LDA-based face recognition algorithms are commonly known as Fisherfaces.\n",
    "\n",
    "Feature-based approaches such as Local Binary Patterns for face recognition have also been proposed and are still heavily used in real-world applications:\n",
    "<image src=\"images/what_is_face_reco_lbps_samples.png\">\n",
    "\n",
    "Deep learning is now responsible for unprecedented accuracy in face recognition. Specialized architectures called siamese networks are trained with a special type of data, called image triplets. We then compute, monitor, and attempt to minimize our triplet loss, thereby maximizing face recognition accuracy.\n",
    "\n",
    "Popular deep neural network face recognition models include FaceNet and OpenFace."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Eigenfaces\n",
    "<image src=\"images/what_is_face_reco_eigenfaces_combo.png\">\n",
    "\n",
    "The Eigenfaces algorithm uses Principal Component Analysis to construct a low-dimensional representation of face images.\n",
    "\n",
    "This process involves collecting a dataset of faces with multiple face images per person we want to identify (like having multiple training examples of an image class we want to identify when performing image classification).\n",
    "\n",
    "Given this dataset of face images, presumed to be the same width, height, and ideally — with their eyes and facial structures aligned at the same (x, y)-coordinates, we apply an eigenvalue decomposition of the dataset, keeping the eigenvectors with the largest corresponding eigenvalues.\n",
    "\n",
    "Given these eigenvectors, a face can then be represented as a linear combination of what Sirovich and Kirby call eigenfaces.\n",
    "\n",
    "Face identification can be performed by computing the Euclidean distance between the eigenface representations and treating the face identification as a k-Nearest Neighbor classification problem — however, we tend to commonly apply more advanced machine learning algorithms to the eigenface representations.\n",
    "\n",
    "If you’re feeling a bit overwhelmed by the linear algebra terminology or how the Eigenfaces algorithm works, no worries — we’ll be covering the Eigenfaces algorithm in detail later in this series of tutorials on face recognition."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## LBPs for face recognition\n",
    "While the Eigenfaces algorithm relies on PCA to construct a low-dimensional representation of face images, the Local Binary Patterns (LBPs) method relies, as the name suggests, on feature extraction.\n",
    "\n",
    "First introduced by Ahonen et al. in their 2004 paper, Face Recognition with Local Binary Patterns, their method suggests dividing a face image into a 7×7 grid of equally sized cells:\n",
    "\n",
    "<image src=\"images/what_is_face_reco_lbps_cells.png\">\n",
    "\n",
    "By dividing the image into cells we can introduce locality into our final feature vector. Furthermore, some cells are weighted such that they contribute more to the overall representation. Cells in the corners carry less identifying facial information compared to the cells in the center of the grid (which contain eyes, nose, and lip structures).\n",
    "\n",
    "Finally, we concatenate the weighted LBP histograms from the 49 cells to form our final feature vector.\n",
    "\n",
    "The actual face identification is performed by k-NN classification using the $\\chi^{2}$ distance between the query image and the dataset of labeled faces — since we are comparing histograms, the $\\chi^{2}$ distance is a better choice than the Euclidean distance.\n",
    "\n",
    "While both Eigenfaces and LBPs for face recognition are fairly straightforward algorithms for face identification, the feature-based LBP method tends to be more resilient against noise (since it does not operate on the raw pixel intensities themselves) and will usually yield better results.\n",
    "\n",
    "We’ll be implementing LBPs for face recognition in detail later in this series of tutorials.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Deep learning-based face recognition\n",
    "<image src=\"images/image-1.jpeg\">\n",
    "\n",
    "Specialized neural network architectures and training techniques, including siamese networks, image triplets, and triplet loss, enabled researchers to obtain face recognition accuracy that was once thought impossible.\n",
    "\n",
    "These methods are far more accurate and robust than previous techniques. And despite the stigma of neural networks being data hungry beasts, siamese networks allow us to train these state-of-the-art models with very little data.\n",
    "\n",
    "If you’re interested in learning more about deep learning-based face recognition, I suggest you read the following guides on PyImageSearch:\n",
    "\n",
    "1. [Face recognition with OpenCV, Python, and deep learning](https://www.pyimagesearch.com/2018/06/18/face-recognition-with-opencv-python-and-deep-learning/)\n",
    "2. [OpenCV Face Recognition](https://www.pyimagesearch.com/2018/09/24/opencv-face-recognition/)\n",
    "3. [Raspberry Pi Face Recognition](https://www.pyimagesearch.com/2018/06/25/raspberry-pi-face-recognition/)\n",
    "4. [Raspberry Pi and Movidius NCS Face Recognition](https://www.pyimagesearch.com/2020/01/06/raspberry-pi-and-movidius-ncs-face-recognition/)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}